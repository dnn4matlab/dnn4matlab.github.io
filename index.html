<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="dnn4matlab.github.io : " />
    <link rel="shortcut icon" href="http://dnn4matlab.github.io/images/favicon.ico" >
    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Deep Neural Networks for Matlab</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/dnn4matlab/code">View on GitHub</a>

          <h1 id="project_title">Deep Neural Networks for Matlab</h1>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>
<a name="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages"><span class="octicon octicon-link"></span></a>Overview</h3>

<p>dnn4matlab provides fast CPU and GPU (CUDA) code to train large neural networks. Code is developed in Matlab, and contains CUDA bindings. With this code
we deliver trained models on ImageNet dataset, which gives top-5 accuracy of 17% on the ImageNet12 validation set. The main purpose of this code is to provide reasonable computer vision models
for people who have little or no experience in field. If you are looking for more comprehensive (but also more complex) frameworks then we recommend <a href="http://www.torch.ch" target="_blank">Torch 7</a>, 
<a href="http://deeplearning.net/software/theano/" target="_blank">Theano</a>, or <a href="http://caffe.berkeleyvision.org/" target="_blank">Caffe</a>. Feel free to use, modify and sell dnn4matlab for any purpose (commercial or academic), but please remember to mention where you got it from.</p>

<p> To get started just checkout our repository:</p>
<pre><code>$ git clone https://github.com/dnn4matlab/code
</code></pre>

Code runs as it is without any compilation on CPU. However, to enable GPU support you have to compile CUDA libraries. Our code supports
only NVIDIA GPUs, and we recommend to use at least GTX 480. To compile code go to the directory <a href="https://github.com/dnn4matlab/code/blob/master/gpu" target="_blank">/gpu</a>
and call <code>make</code>.


<h3><a name="designer-templates" class="anchor" href="#designer-templates"><span class="octicon octicon-link"></span></a>Evaluate ImageNet trained model on your data</h3>

<p>File <a href="https://github.com/dnn4matlab/code/blob/master/Eval.m" target="_blank">/Eval.m</a> shows how to evaluate trained model on your data. It first downloads
trained model from a server, and stores it into <a href="https://github.com/dnn4matlab/code/blob/master/models" target="_blank">/models</a> directory (this is done only once). 
<script src="http://gist-it.appspot.com/github/dnn4matlab/code/blob/master/Eval.m?footer=0"></script>

Evaluating a single image is quite slow in comparison to evaluating a larger batch at the same time. We recommend a batch size of 128 images. 
Code <a href="https://github.com/dnn4matlab/code/blob/master/Eval128.m" target="_blank">/Eval128.m</a> shows how to evaluate 128 images simultaneously. This will take some time the first time you call it because
the code first downloads the entire ImageNet12 validation set (50k images) from our servers.</p>
<script src="http://gist-it.appspot.com/github/dnn4matlab/code/blob/master/Eval128.m?footer=0"></script>

<p>This code displays the prediction error out of 128 images.</p>

<h3><a name="rather-drive-stick" class="anchor" href="#rather-drive-stick"><span class="octicon octicon-link"></span></a>Train your own model</h3>
<p>Files in the directory <a href="https://github.com/dnn4matlab/code/blob/master/plans/" target="_blank">/plans</a> describe various neural network architectures. 
We provide reasonable architectures for MNIST, CIFAR-10, and ImageNet models.</p>
You can use these as templates for your own architectures.

Models available in this package achieve the following performance (you can find current state-of-art at <a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html" target="_blank">here</a>):
</br>
</br>
<center>
<table style="text-align: center;">
  <tr>
    <th>Dataset</th>
    <th>Top-1 error</th> 
    <th>Top-5 error</th> 
    <th>CPU training time</th> 
    <th>GPU training time</th> 
  </tr>
  <tr>
    <td>MNIST</td>
    <td>0.9%</td> 
    <td>no one cares</td> 
    <td>14 mins.</td> 
    <td></td> 
  </tr>
  <tr>
    <td>CIFAR-10</td>
    <td></td> 
    <td>no one cares</td> 
    <td></td> 
    <td></td> 
  </tr>
  <tr>
    <td>ImageNet</td>
    <td></td> 
    <td></td> 
    <td>&infin;</td> 
    <td></td> 
  </tr>
</table>
</center>

<p>The file <a href="https://github.com/dnn4matlab/code/blob/master/Training.m" target="_blank">/Training.m</a> trains MNIST model. It first downloads MNIST dataset.</p>
<script src="http://gist-it.appspot.com/github/dnn4matlab/code/blob/master/Training.m?footer=0"></script>

<p>Models are stored in directory <a href="https://github.com/dnn4matlab/code/blob/master/models" target="_blank">/models</a>. Calling 
<code>Plan('mnist');</code>
loads the most recently trained model. To start from scratch just remove trained model file models/mnist.mat.</p>


<p>If you want to train a CIFAR model uncomment the line <code>Plan('cifar');</code>. To train ImageNet model you have to download training data from 
<a href="http://www.image-net.org/" target="_blank">Image-Net website</a>.
Please place entire training dataset in <a href="https://github.com/dnn4matlab/code/blob/master/data/imagenet" target="_blank">/data/imagenet</a> directory. Then uncomment line
<code>Plan('imagenet');</code> and start training.
Training takes around 6 days on NVIDIA Titan GPU. You need at least GTX 480 to train this model. Training on a CPU is not recommended as it would take ages.</p>

<h3><a name="rather-drive-stick" class="anchor" href="#rather-drive-stick"><span class="octicon octicon-link"></span></a>Internals</h3>
<p>Entire model is contained in a global variable called <b>Plan</b>. To access it call <code>global plan</code>.
All the layer modules are stored in a directory <a href="https://github.com/dnn4matlab/code/blob/master/layers" target="_blank">/layers</a>, and code to verify their correctness is a file <a href="https://github.com/dnn4matlab/code/blob/master/tests/GC.m" target="_blank">/tests/GC.m (gradient checker)</a>. To test a new layer, please extend file <a href="https://github.com/dnn4matlab/code/blob/master/plans/tests.txt" target="_blank">/plans/tests.m</a> and run <a href="https://github.com/dnn4matlab/code/blob/master/tests/GC.m" target="_blank">/tests/GC.m</a>. Moreover, to check that your GPU code is equivalent to your CPU code call <a href="https://github.com/dnn4matlab/code/blob/master/tests/GC_gpu.m" target="_blank">/tests/GC_gpu.m</a>.</p> 


<h3><a name="authors-and-contributors" class="anchor" href="#authors-and-contributors"><span class="octicon octicon-link"></span></a>Authors</h3>

<p>This project is developed by Emily Denton, <a href="http://www.cs.nyu.edu/~zaremba/" target="_blank">Wojciech Zaremba</a>, <a href="http://cims.nyu.edu/~bruna/" target="_blank">Joan Bruna</a>, and <a href="http://cs.nyu.edu/~fergus/" target="_blank">Rob Fergus</a>. If you use this package, please cite:</p>
<pre><code>@article{denton2014exploiting,
 title={Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation},
 author={Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
 journal={arXiv preprint arXiv:1404.0736},
 year={2014}
}
</code></pre>
<center>
<p style="font-weight:bold">Have fun!</p>
</center>

<h3>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
